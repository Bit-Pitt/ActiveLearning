import numpy as np
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.multioutput import MultiOutputClassifier



#formula dell'entropia binaria, uso di numpy per il log
# Metodo .clip: Fa si che valori di < di 1e-10 diventino 1e-10 e viceversa per evitare di avere log2(0) = -inf..
def entropy(p):
    p = np.clip(p, 1e-10, 1 - 1e-10)                    
    return -p * np.log(p) - (1 - p) * np.log(1 - p)

#Seleziona il sample più incerto in base alla media delle entropie.
def select_most_uncertain(model, X_pool):
    y_proba_list = model.predict_proba(X_pool)
    y_proba_matrix = np.vstack([proba[:, 1] for proba in y_proba_list]).T  # shape: (n_samples, n_labels)
    entropies = entropy(y_proba_matrix)
    mean_entropies = entropies.mean(axis=1)
    idx = np.argmax(mean_entropies)
    return idx

#Nota:  y_pool in contesto reale non si hanno, ma noi lo sfruttiamo solo per l'etichettatura automatica (senza rumore) 
#  @algorithm: Ad ogni iterazione: 
#   - calcolo l'incertezza e ottengo il sample con max incertezza, lo inserisco nel train set e lo tolgo dal pool , riaddestro il modello
#  @return il modello finale e il nuovo train e pool set.
def active_learning(model, X_train, y_train, X_pool, y_pool, iterations=100):

    for i in range(iterations):
        print(f"\n=== Iterazione {i+1}/{iterations} ===")

        # Trova l'indice del sample più incerto
        idx = select_most_uncertain(model,X_pool)

        # Aggiungi il sample al train
        X_train = pd.concat([X_train, X_pool.iloc[[idx]]])
        y_train = pd.concat([y_train, y_pool.iloc[[idx]]])

        # Rimuovi il sample dal pool
        X_pool = X_pool.drop(X_pool.index[idx])
        y_pool = y_pool.drop(y_pool.index[idx])

        # Allena il modello sul train set corrente
        model.fit(X_train, y_train)

    return model, X_train, y_train, X_pool, y_pool